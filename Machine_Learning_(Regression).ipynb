{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning (Regression)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suyash091/japanese-sentiment-analysis/blob/master/Machine_Learning_(Regression).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w1z61oo7Xkf"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwyAHm86IXV6",
        "outputId": "e3e314ec-cdae-4aac-c8f9-0e3dddfc985c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "!pip install fugashi[unidic-lite]\n",
        "!python -m unidic download\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fugashi[unidic-lite]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/0c/d0bf73e1a90aeb3e696c7741a812d4b86adc31a8a9783cc92c535ae29016/fugashi-1.0.4-cp36-cp36m-manylinux1_x86_64.whl (476kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 4.5MB/s \n",
            "\u001b[?25hCollecting unidic-lite; extra == \"unidic-lite\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/d2/a4233f65f718f27065a4cf23a2c4f05d8bd4c75821e092060c4efaf28e66/unidic-lite-1.0.7.tar.gz (47.3MB)\n",
            "\u001b[K     |████████████████████████████████| 47.3MB 65kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.7-cp36-none-any.whl size=47556594 sha256=cb9833098b84dd042edf181d50d683f22cd2290942ee58c6523ef19f90f609e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/82/7d/086724645e33a575aafd0b1dae2835c37d2c00c6a0a96ee3a0\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite, fugashi\n",
            "Successfully installed fugashi-1.0.4 unidic-lite-1.0.7\n",
            "/usr/bin/python3: No module named unidic\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/06/2aeff86243c88580ccf78b136d403ce5e0a1eed9091103157f01e806499f/mecab_python3-1.0.1-cp36-cp36m-manylinux2010_x86_64.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5MB 4.5MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOnGHHpDJJKV",
        "outputId": "d58e10f4-04a6-4151-ef5e-6ed5ad1d09c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "!pip install janome==0.3.4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting janome==0.3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/4e/970b49308c6d85a5c33a154823a4a154f427aaba96d306b87bdc0d0f8547/Janome-0.3.4.tar.gz (20.0MB)\n",
            "\u001b[K     |████████████████████████████████| 20.0MB 1.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: janome\n",
            "  Building wheel for janome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for janome: filename=Janome-0.3.4-cp36-none-any.whl size=20682717 sha256=47bb5cbd57177e0ef4238d5ae1ec13b18d6b308218e081e0a187733483df1a02\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/bd/74692e48da56b5d85eb6fd0660ae0bc9b28845dd4543b4f8c1\n",
            "Successfully built janome\n",
            "Installing collected packages: janome\n",
            "Successfully installed janome-0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE3SWzppJSKZ"
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.charfilter import *\n",
        "from janome.tokenfilter import *\n",
        "from tqdm import tqdm\n",
        "char_filters = [UnicodeNormalizeCharFilter(), RegexReplaceCharFilter(u'蛇の目', u'janome')]\n",
        "tokenizer = Tokenizer()\n",
        "token_filters = [CompoundNounFilter(), POSStopFilter(['記号','助詞']), LowerCaseFilter()]\n",
        "a = Analyzer(char_filters, tokenizer, token_filters)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1entfnPf5am",
        "outputId": "04e98941-efdd-428f-cfdb-c6a33e65d645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJLzwibGAwhW"
      },
      "source": [
        "main=pd.read_csv('/content/drive/My Drive/CompleteTwitterdata.csv')[['Tweet','Sentiment','Label']]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsL2Onx72gNx"
      },
      "source": [
        "mainalt=pd.read_csv('/content/TwitterReview (8).csv')[['Tweet','Sentiment','Label']]\n",
        "main=pd.concat([main,mainalt])\n",
        "main=main.dropna()\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJscbVqIgIix",
        "outputId": "6c74cfaa-1fca-432b-e743-b204ef49eb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(main)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "919290"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX63ZeMhJk11"
      },
      "source": [
        "del datalist, labelist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nLjs0xTgCNb"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import MeCab\n",
        "import os\n",
        "url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
        "response = requests.get(url)\n",
        "stopwords = [w for w in response.content.decode().split('\\r\\n') if w != '']\n",
        "\n",
        "tagger = MeCab.Tagger('/usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
        "target_parts_of_speech = ('名詞')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pimCNZRcVXL"
      },
      "source": [
        "def cleantext(text):\n",
        "  text=' '.join(text)\n",
        "  return re.sub(' +', ' ', re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text).replace('https://', ''))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6a3Y4tUmYJu"
      },
      "source": [
        "def cleantext(text):\n",
        "  return re.sub(r\"http\\S+\", \"\", text)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCGGYg9ObdqC"
      },
      "source": [
        "def fetch_tokenize(text):\n",
        "    tokenized_text = []\n",
        "    for chunk in [token.surface for token in a.analyze(text)]:\n",
        "      if chunk not in stopwords:\n",
        "          tokenized_text.append(chunk.lower())\n",
        "    return cleantext(' '.join(tokenized_text))\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E1u6xyogjc1",
        "outputId": "2c06aad0-1b22-456c-c235-c5f9c63e25f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "main.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>✅ポイント高還元 #スーパーDEAL https://t.co/7LROwRwpBL ℹ️4...</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@shinyaFK Twitterキャンペーンの結果は…残念 マイナポイントは電子マネーWA...</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>【 8/10(月)先行予約開始 】ニット 長袖 秋 冬 春 レディース プルオーバー トップ...</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@harukanaworld1 Twitterキャンペーンの結果は…残念 マイナポイントは電...</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>300円クーポン進呈ポイント2倍https://t.co/zygGL2iwMOミント香り付ク...</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Tweet  Sentiment     Label\n",
              "0  ✅ポイント高還元 #スーパーDEAL https://t.co/7LROwRwpBL ℹ️4...        0.2  Positive\n",
              "1  @shinyaFK Twitterキャンペーンの結果は…残念 マイナポイントは電子マネーWA...       -0.3  Negative\n",
              "2  【 8/10(月)先行予約開始 】ニット 長袖 秋 冬 春 レディース プルオーバー トップ...        0.2  Positive\n",
              "3  @harukanaworld1 Twitterキャンペーンの結果は…残念 マイナポイントは電...       -0.2  Negative\n",
              "4  300円クーポン進呈ポイント2倍https://t.co/zygGL2iwMOミント香り付ク...        0.4  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6qBmeVJb5NH"
      },
      "source": [
        "main['body_wakati'] = main.Tweet.apply(fetch_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h96KYtCnDNM7",
        "outputId": "19e444b5-aa01-4932-e004-29086904ecdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataaltseries=[]\n",
        "for text in tqdm(alttokenlist[:3481]+[alttokenlist[3480]]+alttokenlist[3482:]):\n",
        "  dataaltseries.append(cleantext(text))\n",
        "\n",
        "dataseries=[cleantext(text) for text in tokenlist]\n",
        "\n",
        "dftweets = pd.DataFrame({'tweets':dataseries+dataaltseries})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 88366/88366 [00:01<00:00, 57045.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBbyiVtJEUj9",
        "outputId": "91872abc-38bd-41e9-da2a-7da4aa9041bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "max_fatures = 50000\n",
        "tokenizer = Tokenizer(num_words=50000,split=' ')\n",
        "tokenizer.fit_on_texts(dftweets['tweets'].values)\n",
        "X1 = tokenizer.texts_to_sequences(dftweets['tweets'].values)\n",
        "X1 = pad_sequences(X1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-93cdb8764c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_fatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdftweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdftweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dftweets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9zfJBV7aRaT"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# saving\n",
        "#with open('tokenizerscore.pickle', 'wb') as handle:\n",
        "#    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# loading\n",
        "with open('/content/drive/My Drive/tokenizerscore.pickle', 'rb') as handle:\n",
        "    tokenizer0 = pickle.load(handle)\n",
        "with open('/content/drive/My Drive/X1.pickle', 'rb') as handle:\n",
        "    x1 = pickle.load(handle)\n",
        "with open('/content/drive/My Drive/y.pickle', 'rb') as handle:\n",
        "    y = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4InXD91SqfJ5"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# saving\n",
        "with open('X1.pickle', 'wb') as handle:\n",
        "    pickle.dump(X1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# saving\n",
        "with open('y.pickle', 'wb') as handle:\n",
        "    pickle.dump(y, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# loading\n",
        "#with open('tokenizerscore.pickle', 'rb') as handle:\n",
        "#    tokenizer0 = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2yOf3Iud7E7",
        "outputId": "923bcf51-2f5f-4341-db61-6591d5194d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(y)==len(X1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j2UkjNxeHBz",
        "outputId": "1958cc6c-5a1b-4cfd-ce2c-f77719942f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "y=main['Sentiment'].values.tolist()+mainalt['Sentiment'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-248f72859d49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmainalt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxxrPQe3b8LC"
      },
      "source": [
        "max_fatures = len(tokenizer0.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2NJKJSkTE6_",
        "outputId": "fb7a16e2-ac3d-4a71-ed5c-aaee6bc59125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_fatures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "770758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxdqGFSbuP4Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfluwfAPJ3LL",
        "outputId": "96a2456a-04e3-4e5d-d5fb-30f0cd5c2371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "embed_dim = 150\n",
        "lstm_out = 256*2\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X1.shape[1]))\n",
        "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(1,activation='linear'))\n",
        "model.compile(loss = 'mean_absolute_error', optimizer='adam',metrics = ['mean_squared_error'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-972d9c4e2a5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_fatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYls_b8AW4kh"
      },
      "source": [
        "\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "def define_model(length, vocab_size):\n",
        "\tinputs = Input(shape=(length,))\n",
        "\t# channel 1\n",
        "\tembedding1 = Embedding(vocab_size, 100)(inputs)\n",
        "\tconv1 = Conv1D(filters=32, kernel_size=2, activation='relu')(embedding1)\n",
        "\tdrop1 = Dropout(0.25)(conv1)\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "\tflat1 = Flatten()(pool1)\n",
        "\t# channel 2\n",
        "\tembedding2 = Embedding(vocab_size, 100)(inputs)\n",
        "\tconv2 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding2)\n",
        "\tdrop2 = Dropout(0.25)(conv2)\n",
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "\tflat2 = Flatten()(pool2)\n",
        "\t# channel 3\n",
        "\tembedding3 = Embedding(vocab_size, 100)(inputs)\n",
        "\tconv3 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding3)\n",
        "\tdrop3 = Dropout(0.25)(conv3)\n",
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "\tflat3 = Flatten()(pool3)\n",
        "\t# merge\n",
        "\tmerged = concatenate([flat1, flat2, flat3])\n",
        "\t# interpretation\n",
        "\tdense1 = Dense(128, activation='relu')(merged)\n",
        "\toutputs = Dense(1, activation='linear')(dense1)\n",
        "\tmodel = Model(inputs=[inputs], outputs=outputs)\n",
        "\t# compile\n",
        "\tmodel.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_squared_error'])\n",
        "\t# summarize\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKJrhgqOe5e-",
        "outputId": "ae591e2b-37cb-4049-971d-fc309e7a7750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(824674, 60)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vq0DKfFfGOB"
      },
      "source": [
        "import numpy as np\n",
        "X1=[]\n",
        "for x in x1:\n",
        "  X1.append(x.tolist()[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzImXq_JvPug"
      },
      "source": [
        "X1=np.array(X1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a_-W0bzvY3T",
        "outputId": "6df460e0-0a09-4df4-e5db-0152ec731393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "X1[0:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[     0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,  11641,    590,    893,    173,\n",
              "           298,   9355,  13992],\n",
              "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0, 182379,     24,     20,     21,     11,     23,      3,\n",
              "            22,      8,     16,      5,     10,      7,      9,     17,\n",
              "             3,     18,     19,      8,     15,      5,     10,      7,\n",
              "             9,     31,    129],\n",
              "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,    158,     90,    498,  66544,   5857,\n",
              "          1496,   1636,   3616,   2305,    188,   6723,   1056,   6553,\n",
              "          2873,   1694,   5052,   4492,  94048,   7725,   5857,   3412,\n",
              "         34978,    976, 110443]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys3-wuhMSr8Y",
        "outputId": "ee80429b-a73a-4f3b-8838-bec8f80504a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        }
      },
      "source": [
        "\n",
        "model = define_model(X1.shape[1], max_fatures)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 59)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 59, 100)      77075800    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 59, 100)      77075800    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 59, 100)      77075800    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 58, 32)       6432        embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 56, 32)       12832       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 54, 32)       19232       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 58, 32)       0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 56, 32)       0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 54, 32)       0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 29, 32)       0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 28, 32)       0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 27, 32)       0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 928)          0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 896)          0           max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 864)          0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 2688)         0           flatten[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          344192      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 231,610,217\n",
            "Trainable params: 231,610,217\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB6BqnueX_Hv"
      },
      "source": [
        "ylist=[]\n",
        "for i in y:\n",
        "  if i>=0:\n",
        "    ylist.append(1)\n",
        "  else: \n",
        "    ylist.append(0)\n",
        "ylist=np.array(ylist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4QH_zgnUOiz",
        "outputId": "d7d14660-f193-46dd-d48e-1c932e859269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "source": [
        "batch_size = 1024\n",
        "import keras\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/models/sentimentscore.h5',\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "model.fit(X1, np.array(y), validation_split = 0.05, epochs = 50, batch_size=batch_size, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "766/766 [==============================] - 1572s 2s/step - loss: 0.1237 - mean_squared_error: 0.0368 - val_loss: 0.1624 - val_mean_squared_error: 0.0483\n",
            "Epoch 2/50\n",
            "766/766 [==============================] - 1558s 2s/step - loss: 0.0879 - mean_squared_error: 0.0203 - val_loss: 0.1551 - val_mean_squared_error: 0.0454\n",
            "Epoch 3/50\n",
            "766/766 [==============================] - 1550s 2s/step - loss: 0.0711 - mean_squared_error: 0.0140 - val_loss: 0.1536 - val_mean_squared_error: 0.0448\n",
            "Epoch 4/50\n",
            "766/766 [==============================] - 1561s 2s/step - loss: 0.0606 - mean_squared_error: 0.0106 - val_loss: 0.1539 - val_mean_squared_error: 0.0453\n",
            "Epoch 5/50\n",
            "766/766 [==============================] - 1609s 2s/step - loss: 0.0539 - mean_squared_error: 0.0087 - val_loss: 0.1577 - val_mean_squared_error: 0.0473\n",
            "Epoch 6/50\n",
            "526/766 [===================>..........] - ETA: 8:29 - loss: 0.0491 - mean_squared_error: 0.0074"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ddacb42a159c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     save_best_only=True)\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntu6mVLAK_5R"
      },
      "source": [
        "model.save('lstmmodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfYY7psFq1Fh"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X1, np.array(y), test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EX9c41ZsFH6",
        "outputId": "e077d7bf-7a41-4433-941a-5233514ce22a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "min(y_pred.tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-8268964.5479337545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUMDPpjCrivv"
      },
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "lasso001 = Lasso(alpha=0.0001, max_iter=10e5)\n",
        "lasso001.fit(x_train,y_train)\n",
        "y_pred=lasso001.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1F-cvgor7Uk",
        "outputId": "1c2f7d67-e5c8-49cf-90d9-ffa87cac91c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15105568508262415"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2604VRIqLPFh"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import chardet\n",
        "import pandas as pd\n",
        "\n",
        "with open('/content/TestDataForSentimentAnalysisUTF.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())  # or readline if the file is large\n",
        "\n",
        "twitterdata1=pd.read_csv('/content/TestDataForSentimentAnalysisUTFHeader.csv',encoding = result['encoding'],error_bad_lines=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B7Iqw4rLbqf",
        "outputId": "a61bf803-2706-4582-d52b-1d2387b6e28f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "#twitterdata1=twitterdata1[['Tweet','Sentiment','Label']]\n",
        "#twitterdata1=twitterdata1.dropna()\n",
        "twitterdata1.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>楽天の自社グループも厳しい状況の様なので、米配車サービス大手リフトよりもソッチに集中するのは...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#楽天#詐欺メール 楽天市場から支払い情報が一致してないってメール来た酔ってる時に間違って入...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>先月の8日から始めた楽天Room、Aランクになってた✨✨欲しいものありすぎて困る。あまむら食...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Tweet\n",
              "0  楽天の自社グループも厳しい状況の様なので、米配車サービス大手リフトよりもソッチに集中するのは...\n",
              "1  #楽天#詐欺メール 楽天市場から支払い情報が一致してないってメール来た酔ってる時に間違って入...\n",
              "2  先月の8日から始めた楽天Room、Aランクになってた✨✨欲しいものありすぎて困る。あまむら食..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPKflsyGLuEs"
      },
      "source": [
        "testlist=twitterdata1.Tweet.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oky0UrpYL4gG",
        "outputId": "caa45704-7baf-4f0a-efcd-becaa816d51a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "testtokenlist=[]\n",
        "for i in tqdm(testlist):\n",
        "  testtokenlist.append([token.surface for token in a.analyze(i)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6954/6954 [00:44<00:00, 155.88it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVKXTrEUQs2p"
      },
      "source": [
        "Sentiments=twitterdata1.Sentiment.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56Y5nQMDL4jX",
        "outputId": "13be1eb7-15a1-4227-e29e-adb20949da6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "datatestseries=[]\n",
        "for text in tqdm(testtokenlist):\n",
        "  datatestseries.append(cleantext(text))\n",
        "\n",
        "dftesttweets = pd.DataFrame({'tweets':datatestseries})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6954/6954 [00:00<00:00, 77527.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnWaGSmxRS_c",
        "outputId": "9067ac0b-710d-4cc5-acd1-aa3bf5bac53b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "datatestseries"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5 つく 日 日田天領水 まとめ買い 安い 店 順に 日田天領水 12l×2箱楽天市場 天然活性水素水 [楽天] ',\n",
              " 'マラソン 終わっ た また 来る でも 今 欲しい ! もの 売り切れる 前 買い まっしょう❤ 私 服 欲しい ... 楽天市場 [楽天] #rakuafl',\n",
              " '頂い た いい リツイート 数 皆さん 呉 愛 感じ まし た (* ▽`*)お世話 なっ いる 人 贈り物 ちょっとした 自分 ご 褒美 良い 思い ます 釜めし系以外 色々 選べ ます ♪... ',\n",
              " 'レビュー 欲しい 人 必見!レビュー 書ける 人 必見!【ガチンコ モニターズ ━━━ !!!!!★amazon 楽天市場 ヤフーショッピング#レビュー募集',\n",
              " 'レビュー 欲しい 人 必見!レビュー 書ける 人 必見!【ガチンコ モニターズ ━━━ !!!!!★amazon 楽天市場 ヤフーショッピング#レビュー募集',\n",
              " 'レビュー 欲しい 人 必見!レビュー 書ける 人 必見!【ガチンコ モニターズ ━━━ !!!!!★amazon 楽天市場 ヤフーショッピング#レビュー募集',\n",
              " '楽天市場 安心保証付き中古パソコン ガイドブック&office付 初心者 すぐ 使える !初期設定不要 ノートパソコン 中 ... ベストセラー! ',\n",
              " 'nhk 快眠 良い 紹介 さ れ た そう 楽天市場 楽天ランキング1位入賞!】 cloud b クラウドビー アクアタートル 睡眠tranquil turtle 子守唄 ぬいぐるみ 出産祝い [楽天]... ',\n",
              " '北海道 一年 一番 美味しい 時期 獲 れ た 本 いくら 薄皮イクラ 醤油漬け 80g 軍艦 約 10個分 秋鮭卵 魚 お 取り寄せ グルメ 高級海鮮 一人前 お うち時間 国産 食べ物 お 試し 防腐剤等 無 添加 復興(ふっこう) お うち時間応... ',\n",
              " 'おはよう ござい ます 本日 ブログ更新 です ✊✊✊ここ数日 楽天市場 spu 取り上げる 事 多かっ た 今日 です 9種類 あっ 値段 安い 楽天カード 超 かんたん保険 多く 人 spu 還元分 得 できる 思い ます ... ',\n",
              " '== (● ∀`) ┃楽天市場ドア┃- コスプレ♪価格:710円 !?80dタイツ単品 コスプレ 可愛い アレンジ アクセサリー ランジェリー セクシー ハロウィン イベント 余興 仮 ',\n",
              " '最近 ドライヤー 高 品質 で あり 価格帯 かなり 安く なり まし た 定価 2万円以上 し ます 楽天市場 かなり 安く 購入 できる コチラ オススメ です !ホリスティックキュア 気 なる 予算 抑え たい 方 低 価格 良い ドライヤ... ',\n",
              " '楽天市場 すぐ 在庫切れ...✴ ポポロ 2020年 11 月号✴ amazon なら まだ 買える !⬇ こちら ⬇ 早い 者勝ち!#平野紫耀 #永瀬廉 #髙橋海人... ',\n",
              " 'レビュー 欲しい 人 必見!レビュー 書ける 人 必見!【ガチンコ モニターズ ━━━ !!!!!★amazon 楽天市場 ヤフーショッピング#レビュー募集',\n",
              " 'レビュー 欲しい 人 必見!レビュー 書ける 人 必見!【ガチンコ モニターズ ━━━ !!!!!★amazon 楽天市場 ヤフーショッピング#レビュー募集',\n",
              " '楽天市場 在庫 あり ます 超 快適マスク プリーツタイプふつう(30枚)日本製 ユニ・チャーム ウイルス飛沫 花粉 侵入 防ぎ ます かぜ pm2.5 安心 4903111951376 ふつう サイズ3980円... ',\n",
              " 'レビュー 欲しい 人 必見!レビュー 書ける 人 必見!【ガチンコ モニターズ ━━━ !!!!!★amazon 楽天市場 ヤフーショッピング#レビュー募集',\n",
              " '分かる ~、男の子 ピンク 着せ たい し マイルド な 色 欲しい ボーダー恐竜宇宙電車動物 飽き た ん !!!最近 柄 ない シンプル な の ワンポイント物 着せ てる もっと ない ん です ~!!!dilashさんお願い し ます (楽天市場 いつも お世話 なっ ます )',\n",
              " '楽天市場 スター 星 ネックレス レディース ステンレス シンプル 大人 可愛い ジュエリー ランキング ギフト レディース ステンレス 金属アレルギー対応 スター 星 カラー シンプル... ',\n",
              " '楽天市場 シンプル ピアス レディース プラチナ 加工 スタッド フラワー 誕生日 プレゼント 結婚記念日 女性 彼女 妻 嫁 金属アレルギー ニッケルフリー シンプル 大人 可愛い',\n",
              " '値下げ!1袋あたり 最 安値!アルボースアルボナース 手指消毒剤 詰め替え 用 900ml×16個 ケース販売安心 日本製アルコール消毒液広範囲 微生物 短時間 効力 発揮手 やさしい 3種類 保 湿 剤 ... ',\n",
              " '楽天市場 本 貝パール 真珠 花 スワロフスキー silver925 シルバー925 あす つく レディースアクセサリー プレゼント 彼女 女性 誕生日 プレゼント ギフト クリスマス 大人 可愛い おしゃれ 属アレルギー',\n",
              " '@_ykm9 楽天市場 で よく お 買い物 する 人 なら あっ た ほう 良い です ...!',\n",
              " '楽天市場 ファッション感覚 刺激 する キュート な デザイン ファッションアクセサリー 大人 可愛い らし さ 表現 し くれ ます ブレスレット 誕生石 スワロフスキークリスタル ピンクゴールド k18金rgp 可愛い プレゼント',\n",
              " '楽天市場 そう だ ローソン お節料理 予約 始め てる ん です うか れ これ20年弱 食べ て ない 予約 する お節料 理っ 美味しい の !?',\n",
              " 'レビュー 欲しい 人 必見!レビュー 書ける 人 必見!【ガチンコ モニターズ ━━━ !!!!!★amazon 楽天市場 ヤフーショッピング#レビュー募集',\n",
              " '楽天市場 ブレスレット 金属アレルギー ゴールド 金 メッキ メタル キュービックジルコニア シンプル 重ね 着け 重ね付け 重ね づけ ニッケルフリー 安心 アクセサリー 結婚式 パーティー ギフト レディース ジュエルボックス',\n",
              " 'reika room -「 欲しい ! 出会える 楽天市場 ショッピングsns ',\n",
              " '楽天市場 進撃 巨人 立体機動 リュック リュックサック ブランド アニメ 進撃 巨人 超 軽量 軽い ナイロン バックパック レディース キッズ オシャレ ユニーク メンズ デイパッ... ',\n",
              " '楽天市場 10個セット入り ヘアピン 髪飾り 女の子 キッズ ベビー 赤ちゃん 女児 かわいい ヘアアクセサリー 10個セット ピンク 子供用 ヘアクリップ 髪 留め kids bab... ',\n",
              " '楽天市場 リブニット トップス バルーン袖 ボトルネック リブ ニット セーター レディース 清楚 可愛い バルーンスリーブ ニットセーター ハイネック 長袖 ドルマン フレア バル... ',\n",
              " '#ヒルナンデス 紹介品✨菊正宗 日本酒 化粧水 高保 湿 (500ml*2コセット) ldkビューティー ベストコスメ 選ば れ まし た 安く 大 容量プチプラ 嬉しい 楽天市場⬇ ',\n",
              " '楽天市場 &即納 チュニック 大人 可愛い フレア キャンディスリーブ ニット 上品 起毛 秋冬 レディース リブニット トップス ロング セーター 長袖 リボン 清楚 袖 コンシャス... ',\n",
              " '楽天市場 冷感マスク 在庫 あり 水着素材 花粉対策 洗える マスク 水着マスク 布 洗える 夏用 大人用/ 男性用/女性用 白 ホワイト 黒 ブラック 通気性 個包装 ま すく mask 繰り返し 伸縮性 uvカット おしゃれ 可愛い',\n",
              " '楽天市場 レース ブラウス ボリュームスリーブ ギャザー 上品 長袖 七分袖 薄手 透け 感 ふんわり / 大人 可愛い レディース トップス 刺繍 ボリューム袖 シースルー パンチング... ',\n",
              " '楽天市場 春 秋 新作 レディース トップス 3カラー 長袖 vネック シャツ tシャツ おしゃれ ブラウス 可愛い 無地シンプル 女性 成人 通勤 上品 体型カバー 20代 30代 40代 50代 s~xxl',\n",
              " '楽天市場 tシャツ スカート 2点 セットアップ チェック柄 ミモレ丈 ハイウエスト 大人 可愛い 上品 白 黒 / レディース トップス カットソー ワンピース バイカラー 上下 セ... ',\n",
              " '楽天市場 綿麻 パンツ レディース ゆったり 女性パンツ薄手カジュアル ボトムス 快適 少女 学生 可愛い カジュアル パンツ パンツ ロングパンツ 薄 美脚効果抜群 ファッション 普段着 20代 30代 40代 50代',\n",
              " '楽天市場 ゴスロリ ロリータ キッズ ワンピース 女の子 ハロウィン 衣装 ワンピース アニメ cosplay 日系 ロリータ ドレス コスプレ リボン 二次元衣装 少女 可愛い ロ... ',\n",
              " '楽天市場 総 レース ブラウス ボリュームスリーブ 上品 大人 可愛い 長袖 ギャザー 袖コンシャス / レディース トップス シャツ きれいめ ハイネック 花柄 レース 刺繍 襟 なし ... ',\n",
              " '楽天市場 2枚入 新作 5カラー ショーツ パンツ 女性美形ショーツ 下着 引締め もっ ちり肌触り 大人360度すべて 臭い 軽減 抗菌加工 伸縮性 良い 理想 ノンストレスボックスショ... ',\n",
              " 'レビュー 欲しい 人 必見!レビュー 書ける 人 必見!【ガチンコ モニターズ ━━━ !!!!!★amazon 楽天市場 ヤフーショッピング#レビュー募集',\n",
              " '楽天市場 新品 2枚入 2色 各 1枚 髪飾り ハロウイン 女性 大人用 コスプレ 頭飾り ハロウィン頭飾り ハロウイン お 遊戯会 可愛い 満点 お洒落 カラフル 頭飾り ハロウィーン イベント',\n",
              " '楽天市場 3枚組 紳士 大 容量&強力 消 臭 安心ボクサーパンツ同色3枚組 グレー または ブラック 130cc対応 男性用 メンズ 失禁パンツ 下着 過 活動膀胱 重 失禁 介護',\n",
              " '楽天市場 綿混 優しく 安心 深 履き 吸水ショーツ',\n",
              " '楽天市場 コロナ 支援 応援 値下げ メール便 なら レディース 雑貨 かん ざし 簪 二本 挿し uピン ラインストーン 夏 和装 普段使い 祭り 成人式 可愛い 和小物 日本 12 8... ',\n",
              " '楽天市場 メール便 なら レディース 雑貨 かん ざし 簪 玉簪 一本 挿し 夏 和装 普段使い 祭り 成人式 可愛い 和小物 日本 4 87 z a b c d shot ショット [170707]',\n",
              " '楽天市場 コロナ 支援 応援 値下げ メール便 なら レディース 雑貨 かん ざし 簪 二本 挿し 菊 鞠菊 花 uピン ラインストーン 夏 和装 普段使い 祭り 成人式 可愛い カラフル... ',\n",
              " '楽天市場 コロナ 支援 応援 値下げ メール便 なら レディース 雑貨 かん ざし 簪 一本 挿し ストーン パール 夏 カジュアル 和装 普段使い 祭り 成人式 可愛い カラフル 99... ',\n",
              " '楽天市場 コロナ 支援 応援 値下げ メール便 なら レディース 雑貨 かん ざし 簪 一本 挿し ストーン パール 夏 カジュアル 和装 普段使い 祭り 成人式 可愛い カラフル 99... ',\n",
              " '@kumakosan8 楽天市場 ニトリ 通販サイト や ( ー ) ニヤリ 美味しい 正義',\n",
              " '楽天市場 リュック レディース ナイロン 大人 はっ 水 ポケット 多い 本 革 リュックサック レザー ナイロンリュック 大人リュック マザーズリュック ママリュック 撥水 通勤 通学... ',\n",
              " '楽天市場 lizdays リズデイズ リュック レディース 大人 リュックサック リュックマザーズ おしゃれ ショルダーバッグ ハンドバッグ トートバッグ 通勤 通学 可愛い 大 容量... ',\n",
              " '楽天市場 クール素材 ベルト付き ワンピース レディース ノースリーブ 涼しい きれいめ カジュアル 夏 冷感 バックスリット 2way 大人 可愛い ロング丈 30代 40代 50代 メール便対応可',\n",
              " '楽天市場 スプリングコート トレンチコート レディース テロンチ アウター ドルマン スリーブ mサイズ 大きい サイズ ロング ショート 春コート 可愛い 30代 40代 50代 コート きれいめ カジュアル',\n",
              " 'クマ アイスキューブトレイ 可愛い 氷 作れ い ます コーヒー カフェオレ コーラ 氷 作っ たら 可愛く なり そう です ♡#熊 #アイス #シリコン #アイスキューブトレイ #氷 #トレイセット #カフェ #くま... ',\n",
              " '楽天市場 多い 日 寝る 時 安心 丈 長い サニタリーショーツ',\n",
              " '楽天市場 長袖 カットソー チュニック ワンピース レディース トップス 秋冬 tシャツ ドロップショルダー ミディアム丈 シンプル ゆったり 大きめ 大きい サイズ サイドスリット... ',\n",
              " '楽天市場 在庫 あり ます 超 快適マスク プリーツタイプふつう(30枚)日本製 ユニ・チャーム ウイルス飛沫 花粉 侵入 防ぎ ます かぜ pm2.5 安心 4903111951376 ふつう サイズ3980円... ',\n",
              " '楽天市場 財布 レディース 長 財布 おしゃれ 可愛い 人気 長 サイフ 使い やすい 軽量 コンパクト カラバリ豊富 メール便 2020春夏新作 フリー ali-msq880005 即納 2-5日 メ込',\n",
              " '楽天市場 財布 レディース 長 財布 おしゃれ 可愛い 人気 長 サイフ コンパクト 軽い 使い やすい カラバリ豊富 メール便 2020春夏新作 ali-ywpt839 予約販売 15-20日 メ込',\n",
              " '楽天市場 クロスストラップ パンプス 痛く ない レディース ポインテッドトゥ ローヒール ストラップ きれいめ 型 押し 上品 大人 仕事 オフィス 美脚 シューズ おしゃれ 可愛い ... ',\n",
              " '楽天市場 ルームウェア ネコポス便不可 お 花プリント 可愛い ベロア ガウン シンプル キャミソール ワンピース セット レディース 長袖 ピンク ポイント消化[r001-rw-0293]',\n",
              " '楽天市場 sale/28%off メール便 ビジュードール フリンジチャーム カラフルタッセル ビジュー 人形 キーホルダー バッグアクセ 可愛い レディース レデイース キ-ホルダ... ',\n",
              " '楽天市場 ラップ ワイドパンツ レディース アシンメトリー ガウチョパンツ ワイド ガウチョ ゆったり 楽ちん 無地 ボトムス 可愛い ブラック メール便 2020春夏新作 m/l lgww-at3538 即納 2-5日 メ込',\n",
              " '楽天市場 ワンピース レディース 冷感素材 ロング丈 マキシ丈 ノースリーブ ノースリ とろみ aライン 大人 自宅 おしゃれ 可愛い ブラック メール便 2020春夏新作 フリー lgww-at3618 即納 2-5日 メ込',\n",
              " '楽天市場 カシュクール ニット セーター 長袖 vネック レディース トップス 大人シンプル 仕事 オフィス おしゃれ 可愛い ブラック ホワイト カラバリ豊富 2020秋冬新作 lgww-at3358 予約販売 15-20日 メ込',\n",
              " '楽天市場 2way ウィングチップ シューズ レディース バブーシュ ローヒール レースアップ マニッシュシューズ カジュアル マニッシュ 美脚 シューズ おしゃれ 可愛い キレイ... ',\n",
              " '楽天市場 チュニック レディース 長袖 トップス 可愛い ブラック メール便 2020春夏新作 m/l/xl/2xl o-au105 予約販売 15-20日 メ込',\n",
              " '楽天市場 ローファー パンプス 痛く ない レディース ヒール キルト シューズ ローヒール エナメル カジュアル おしゃれ 可愛い キレイ 疲れ ない 2020秋冬新作 gt-2018 予約販売 15-20日 宅込',\n",
              " '楽天市場 残り わずか 在庫限り 超 価格 ギャザー ブラウス シャツ レディース 半袖 ゆったり トップス 大人 カジュアル 無地 可愛い ホワイト メール便 2020春夏新作 フリー lgww-at2328 即納 2-5日 メ込',\n",
              " '楽天市場 水着 レディース ビキニ リバーシブル 上下2点セット ノン ワイヤー パット入り ブルー マルチカラー m l レギュラーパンツ 三角ビキニ フリルビキニ マルチボーダー柄 無地 大人 可愛い 海 プール メール便y',\n",
              " '楽天市場 水着 レディース ビキニ ハイウエスト ショーツ 上下別柄 スポブラ ノン ワイヤー パット入り ネイビー サーモンピンク 無地 チェック柄 m l シンプル 女性 レトロ 可愛い スポーツブラ 海 プール メール便y',\n",
              " '楽天市場 水着 レディース ビキニ ビスチェ ノン ワイヤー パット内蔵 くすみ ピンク ブルー m l 無地 編み上げ チラ 見せ 長 フレア 体型カバー ワンピース風 可愛い ハイウエスト レトロ 海 ビーチ 女性 メール便y',\n",
              " 'レビュー 欲しい 人 必見!レビュー 書ける 人 必見!【ガチンコ モニターズ ━━━ !!!!!★amazon 楽天市場 ヤフーショッピング#レビュー募集',\n",
              " '楽天市場 結婚式 ドレス ワンピース 二次会 フォーマル 膝丈 パーティードレス 可愛い 長袖 袖 あり 袖付き プリーツ シフォン 食事 お呼ばれ 大人 フォーマルドレス レディース... ',\n",
              " '楽天市場 9/17 13:59 累計販売数5700本突破 楽天ランキング1位入賞 クリックポスト クロスリボンバナナクリップ 可愛い 甘 すぎ ない 今 人気 シンプルバナナクリップ',\n",
              " '楽天市場 帽子 レディース 大きい サイズ | 夏 春 日焼け 麦わら 大きい リボン uvカット ハット 麦わら 母 日 オシャレ 高級感 ギフト 贈り物 熱中症 日 日除け 防... ',\n",
              " 'ところで 福服楽天市場 70%off やっ ます 限定sale な 見 いら し 下さい ♪ 良い もの 安く get チャンス です ! #福服 ',\n",
              " '楽天市場 ミュール サンダル ローファー レディース おしゃれ 可愛い 靴 シューズ 疲れ ない ブラック 2020春夏新作 35/36/37/38/39 wm-520 予約販売 15-20日 宅込',\n",
              " '楽天市場 ローファー スクエアトゥ パンプス レディース ぺたんこ フラット 美脚 シューズ ローヒール 歩き やすい シンプル おしゃれ 可愛い キレイ 疲れ ない ブラック 2020... ',\n",
              " '楽天市場 ワンハンドルバッグ ハンドバッグ レディース 小さめ スモールバッグ おしゃれ 可愛い 鞄 バッグ ブラック ホワイト 2020春夏新作 フリー tb-tosd65 予約販売 15-20日 宅込',\n",
              " '楽天市場 ニット セーター ハイネック レディース ショルダーボタン ボタン 長袖 あっ た トップス シンプル 無地 可愛い ブラック 2020秋冬新作 フリー lgww-at2589 即納 2-5日 宅込',\n",
              " '楽天市場 ショルダーバッグ レディース 小さめ 斜め かけ ポシェット おしゃれ 可愛い 肩掛け 鞄 バッグ ブラック ホワイト カラバリ豊富 2020春夏新作 フリー ali-gy628 予約販売 15-20日 宅込',\n",
              " '楽天市場 かごバッグ お洒落 女性 キレイ 夏 サマー シンプル ナチュラル レディース おしゃれ 可愛い 鞄 バッグ 2020春夏新作 フリー lgww-at2516 予約販売 15-20日 宅込',\n",
              " '楽天市場 トートバッグ レディース シンプル a4 仕事 通勤 通学 オフィス おしゃれ 可愛い 鞄 バッグ ブラック ホワイト 2020春夏新作 フリー alikb-2006 予約販売 15-20日 宅込',\n",
              " '楽天市場 ミュール サンダル レディース ぺたんこ 3type フラットサンダル おしゃれ ローヒール 可愛い 靴 キラキラ シューズ 疲れ ない ゴージャス エレガント パーティ 上... ',\n",
              " '楽天市場 残り わずか 在庫限り 超 価格 ワンピース レディース マキシ丈 ロング丈 おしゃれ 可愛い ブラック ホワイト 大きい サイズ メール便 2020春夏新作 m/l/2l xl... ',\n",
              " '楽天市場 スカート 異 素材 ロング レディース マキシ丈 ロングスカート ボトムス 楽ちん おしゃれ 可愛い ブラック ホワイト メール便 2020春夏新作 フリー 0856-d230a 予約販売 15-20日 メ込',\n",
              " '楽天 みんな大好き 魚卵 ランキング 第 10位 限定20個 在庫 いくら イクラプチプチ はじける 鮭イクラ 国産 北海道 築地市場 豊洲市場 いくら 醤油漬け500g 美味しい お 中 ...評価4.9レビュー10件... ',\n",
              " '楽天市場 ローファー パンプス 痛く ない レディース ローヒール ヒール キルト シューズ 厚底 カジュアル おしゃれ 可愛い キレイ 疲れ ない 2020秋冬新作 gt-dxb4 予約販売 15-20日 宅込',\n",
              " '楽天市場 ブラジャー 下着 レディース 単品 おしゃれ 可愛い キレイ ブラック ホワイト メール便 2020春夏新作 フリー ali-zj14 予約販売 15-20日 メ込',\n",
              " '楽天市場 アシメネック ニット セーター vネック レディース 長袖 アシンメトリー シンプル 大人 トップス おしゃれ 可愛い 2020秋冬新作 lgww-at3374 予約販売 15-20日 メ込',\n",
              " '楽天市場 残り わずか 在庫限り 超 価格 バックレースアップ サテン パンツ ワイドパンツ レディース 楽ちん ゆったり 大人 ボトムス トレンド おしゃれ 可愛い メール便 2020春... ',\n",
              " '楽天市場 残り わずか 在庫限り 超 価格 ドローストリング ワンピース レディース リブ ロングワンピース vネック マキシ丈 ロング リラックス 大人 ウエストゴム おしゃれ 可愛い ... ',\n",
              " '楽天市場 tシャツ レディース 半袖 チュニック ゆったり 10枚組 セット おまかせ 夏 可愛い アニマル 総 柄 m-l',\n",
              " '楽天市場 在庫 あり ます 超 快適マスク プリーツタイプ 小さめ(30枚)日本製 ユニ・チャーム ウイルス飛沫 花粉 侵入 防ぎ ます かぜ pm2.5 安心 4903111951208 小さめサイズ3480円... ',\n",
              " '楽天市場 プリントボウタイブラウス ボウタイ ブラウス フォーマル 通勤服レディース トップス 長袖 参観日 オフィス シンプル ベーシック フェミニン レトロ 上品 ol 大人カジ... ',\n",
              " '楽天市場 リブドルマンニット ニット リブドルマン 大人 冬 秋 無地 ボーダー 大人 可愛い カジュアル 長袖 ボートネック ゆったり 着痩せ 体型カバー',\n",
              " '今日 楽天市場 最近 買っ た お 袖 可愛い トップス 着 気分 良い 日 だっ た ✨ ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmY194RTL5Li"
      },
      "source": [
        "Xtest1 = tokenizer0.texts_to_sequences(dftesttweets['tweets'].values)\n",
        "Xtest1 = pad_sequences(Xtest1,maxlen=59)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhRbZGjxTx3C",
        "outputId": "d46da95f-011d-4791-ae6c-0e92e0147e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xtest1.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6954, 59)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scKY2wcKynIY"
      },
      "source": [
        "y_pred=model.predict(X1[:20000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3aIOKYMyz1Y"
      },
      "source": [
        "import numpy as np\n",
        "Y1_test=np.array(y[:20000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qE847GtUDHo",
        "outputId": "55e6ed9a-4566-479e-8274-4eaa0da4e9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array(Sentiments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.4,  0.4,  0.2, ..., -0.7, -0.5, -0.1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9UFV1Jd08S2",
        "outputId": "8c4a6f92-f7da-410e-ad0d-c181902ec83a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_pred.ravel(), Y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04886331252975739"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piM40WzmL6oz",
        "outputId": "65f6e5f3-0b2d-45f9-e5f7-4270941b4cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "print(y_pred[:100])\n",
        "print(Y1_test[0:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.  -0.   0.8  0.4  0.4  0.4  0.4  0.2  0.3  0.3  0.2  0.6 -0.   0.4\n",
            "  0.4  0.4  0.4  0.4  0.1  0.7  0.2  0.5 -0.   0.8  0.8  0.4  0.4 -0.\n",
            "  0.2  0.1  0.2  0.8  0.2  0.9  0.4  0.8  0.8  0.9  0.6  0.5  0.1  0.4\n",
            "  0.5  0.3  0.2  0.1 -0.   0.5  0.6  0.6  0.1  0.2  0.5  0.9  0.6  0.6\n",
            " -0.   0.5  0.4  0.1  0.1  0.7  0.8  0.4  0.3  0.6  0.1  0.6  0.1  0.2\n",
            "  0.7  0.4  0.4  0.4  0.4  0.2  0.5  0.1  0.3 -0.   0.2  0.2  0.1  0.2\n",
            "  0.3  0.4  0.1  0.1  0.   0.2  0.2  0.1  0.1  0.5  0.6  0.4  0.4  0.4\n",
            " -0.   0.3]\n",
            "[0.  0.  0.8 0.4 0.4 0.4 0.4 0.2 0.3 0.3 0.2 0.6 0.  0.4 0.4 0.4 0.4 0.4\n",
            " 0.2 0.6 0.2 0.4 0.  0.9 0.8 0.4 0.4 0.  0.2 0.1 0.2 0.8 0.4 0.9 0.4 0.9\n",
            " 0.8 0.9 0.6 0.5 0.  0.4 0.5 0.  0.2 0.  0.  0.5 0.6 0.5 0.6 0.4 0.5 0.9\n",
            " 0.7 0.6 0.  0.5 0.4 0.  0.1 0.7 0.8 0.4 0.8 0.8 0.1 0.5 0.1 0.4 0.8 0.4\n",
            " 0.4 0.  0.4 0.2 0.8 0.1 0.3 0.  0.  0.  0.  0.4 0.  0.4 0.1 0.  0.1 0.3\n",
            " 0.9 0.1 0.1 0.4 0.5 0.4 0.4 0.3 0.  0. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI3Ngj6C0PsB",
        "outputId": "4bfc3c63-d5d5-4ae4-cd60-b438438febf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "y_pred=np.around(y_pred.ravel(), decimals=1)\n",
        "mean_absolute_error(y_pred.ravel(), Y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06820987776278253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTqgtjN9VJiG",
        "outputId": "4c08db9c-ac48-4fe5-98b4-11b610ccc313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "model.fit(Xtest1, Y1_test, validation_split = 0.1, epochs = 50, batch_size=batch_size, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - 24s 2s/step - loss: 0.1287 - mean_squared_error: 0.0376 - val_loss: 0.1499 - val_mean_squared_error: 0.0438\n",
            "Epoch 2/50\n",
            "12/12 [==============================] - 24s 2s/step - loss: 0.0972 - mean_squared_error: 0.0248 - val_loss: 0.1470 - val_mean_squared_error: 0.0423\n",
            "Epoch 3/50\n",
            "12/12 [==============================] - 24s 2s/step - loss: 0.0770 - mean_squared_error: 0.0169 - val_loss: 0.1481 - val_mean_squared_error: 0.0428\n",
            "Epoch 4/50\n",
            " 6/12 [==============>...............] - ETA: 10s - loss: 0.0664 - mean_squared_error: 0.0135"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e93eca06ad4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thJyUG2IMbIH",
        "outputId": "3aaaa7e7-f748-464c-b818-31c5a528b019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mean_absolute_error(y_pred.ravel(), Y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0815298311112923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHkUBbYmYe5z",
        "outputId": "5b942354-a03d-427e-ccff-f563db7b9a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "import numpy as np\n",
        "y_pred=np.around(y_pred.ravel(), decimals=1)\n",
        "mean_absolute_error(y_pred.ravel(), Y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-38135c8f8111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY1_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \"\"\"\n\u001b[1;32m    177\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 178\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6954, 12960]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt2FhhYsVqKS"
      },
      "source": [
        "pd.DataFrame({'tweets':datatestseries}).to_csv('tweets.csv')\n",
        "pd.DataFrame({'original_results':Y1_test.tolist()}).to_csv('original_results.csv')\n",
        "pd.DataFrame({'model_results':y_pred.ravel().tolist()}).to_csv('model_results.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0HiaBxoZU-b"
      },
      "source": [
        "twitterdata1['Sentiments']=y_pred.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7R6xVIyaQHh"
      },
      "source": [
        "label=[]\n",
        "for tweet in twitterdata1['Sentiments'].values.tolist():\n",
        "  if tweet>=0:\n",
        "    label.append('Positive')\n",
        "  else:\n",
        "    label.append('Negative')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDBWwHVJaxTI"
      },
      "source": [
        "twitterdata1['Label']=label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA4PHUKsZuZ-",
        "outputId": "1df0ddb0-561c-4a77-f15f-d02ec4c53bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "twitterdata1.tail(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Sentiments</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6904</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/Hs...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6905</th>\n",
              "      <td>愛猫用のサイエンスダイエットシニア用180cmのキッチンマットフェイスマスクデオナチュレのソ...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6906</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/nc...</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6907</th>\n",
              "      <td>今回も買うぞー！お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https:/...</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6908</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/EV...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6909</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/UZ...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6910</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/um...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6911</th>\n",
              "      <td>高性能なワイヤレスイヤホン、中華製のへっぽこなのはもうこりごりっ！お得なクーポンがほしい！ ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6912</th>\n",
              "      <td>#ダイソン掃除機　お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https:...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6913</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/yb...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6914</th>\n",
              "      <td>離乳食が作れるフードプロセッサーと、美味しいお取り寄せグルメを買いたいな〜❤️お得なクーポン...</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6915</th>\n",
              "      <td>毎月１日はワンダフルデーエントリーでポイント3倍24時間限定の一日限りです今月の超目玉商品や...</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6916</th>\n",
              "      <td>子どものオムツ、子どもの秋、冬服、ミルク、子どものシャンプー、ボディソープ、ボディクリームが...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6917</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/6S...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6918</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/G4...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6919</th>\n",
              "      <td>楽天が面白いキャンペーンやってますね！ハッシュタグ　#楽天スーパーSALEでほしいもの 　を...</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6920</th>\n",
              "      <td>果物！訳ありでもいいから大容量！大容量果物詰め合わせとか欲しいお得なクーポンがほしい！ #楽...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6921</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/fs...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6922</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/0Q...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6923</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/NO...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6924</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/xu...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6925</th>\n",
              "      <td>馬刺し！レバ刺し！エアコン！楽天スーパーSALEでほしいものをつぶやこう！ https://...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6926</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/uh...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6927</th>\n",
              "      <td>マキタのスティック掃除機(ガチ)お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6928</th>\n",
              "      <td>楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/tyMXCrSt7...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6929</th>\n",
              "      <td>楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/IGuAWyqEM...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6930</th>\n",
              "      <td>✅ポイント高還元#スーパーDEAL https://t.co/7LROwRwpBL #コンタ...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6931</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/JK...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6932</th>\n",
              "      <td>安心して釣りが出来るように、ライフジャケットが欲しいです！ #楽天スーパーSALEでほしいも...</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6933</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/bg...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6934</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/Po...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6935</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/mW...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6936</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの @RakutenJP http...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6937</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/AS...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6938</th>\n",
              "      <td>富士通ゼネラル 脱臭機 PLAZION ブラック DAS-15K-B がほしーい❣️お得なク...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6939</th>\n",
              "      <td>楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/ReYTtJ27i...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6940</th>\n",
              "      <td>楽天トラベルスーパーSALE開催決定！✅9月4日20時～✅宿クーポン割引率最大28％✅注目は...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6941</th>\n",
              "      <td>✅人気のブランドを安く欲しい✅最近アウトレットモールがいまいちだなと思う方ぜひ見てくださいブ...</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6942</th>\n",
              "      <td>majestouchお得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6943</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/eD...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6944</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/89...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6945</th>\n",
              "      <td>自転車!!楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/Kibm...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6946</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/fL...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6947</th>\n",
              "      <td>楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/daPe7PIgm...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6948</th>\n",
              "      <td>積み木と子供のお洋服ほしい〜！お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの ...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6949</th>\n",
              "      <td>楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/IYFi0BI4m...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6950</th>\n",
              "      <td>ペットモニター！楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/E...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6951</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/XB...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6952</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/2s...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6953</th>\n",
              "      <td>お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/A5...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Tweet  Sentiments     Label\n",
              "6904  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/Hs...         0.1  Positive\n",
              "6905  愛猫用のサイエンスダイエットシニア用180cmのキッチンマットフェイスマスクデオナチュレのソ...         0.1  Positive\n",
              "6906  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/nc...         0.4  Positive\n",
              "6907  今回も買うぞー！お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https:/...         0.2  Positive\n",
              "6908  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/EV...         0.1  Positive\n",
              "6909  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/UZ...         0.1  Positive\n",
              "6910  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/um...         0.1  Positive\n",
              "6911  高性能なワイヤレスイヤホン、中華製のへっぽこなのはもうこりごりっ！お得なクーポンがほしい！ ...         0.0  Positive\n",
              "6912  #ダイソン掃除機　お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https:...         0.1  Positive\n",
              "6913  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/yb...         0.1  Positive\n",
              "6914  離乳食が作れるフードプロセッサーと、美味しいお取り寄せグルメを買いたいな〜❤️お得なクーポン...         0.4  Positive\n",
              "6915  毎月１日はワンダフルデーエントリーでポイント3倍24時間限定の一日限りです今月の超目玉商品や...         0.2  Positive\n",
              "6916  子どものオムツ、子どもの秋、冬服、ミルク、子どものシャンプー、ボディソープ、ボディクリームが...         0.1  Positive\n",
              "6917  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/6S...         0.1  Positive\n",
              "6918  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/G4...         0.1  Positive\n",
              "6919  楽天が面白いキャンペーンやってますね！ハッシュタグ　#楽天スーパーSALEでほしいもの 　を...         0.4  Positive\n",
              "6920  果物！訳ありでもいいから大容量！大容量果物詰め合わせとか欲しいお得なクーポンがほしい！ #楽...         0.1  Positive\n",
              "6921  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/fs...         0.1  Positive\n",
              "6922  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/0Q...         0.1  Positive\n",
              "6923  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/NO...         0.1  Positive\n",
              "6924  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/xu...         0.1  Positive\n",
              "6925  馬刺し！レバ刺し！エアコン！楽天スーパーSALEでほしいものをつぶやこう！ https://...         0.1  Positive\n",
              "6926  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/uh...         0.1  Positive\n",
              "6927  マキタのスティック掃除機(ガチ)お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの...         0.1  Positive\n",
              "6928  楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/tyMXCrSt7...         0.1  Positive\n",
              "6929  楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/IGuAWyqEM...         0.1  Positive\n",
              "6930  ✅ポイント高還元#スーパーDEAL https://t.co/7LROwRwpBL #コンタ...         0.1  Positive\n",
              "6931  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/JK...         0.1  Positive\n",
              "6932  安心して釣りが出来るように、ライフジャケットが欲しいです！ #楽天スーパーSALEでほしいも...         0.2  Positive\n",
              "6933  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/bg...         0.1  Positive\n",
              "6934  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/Po...         0.1  Positive\n",
              "6935  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/mW...         0.1  Positive\n",
              "6936  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの @RakutenJP http...         0.1  Positive\n",
              "6937  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/AS...         0.1  Positive\n",
              "6938  富士通ゼネラル 脱臭機 PLAZION ブラック DAS-15K-B がほしーい❣️お得なク...         0.1  Positive\n",
              "6939  楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/ReYTtJ27i...         0.1  Positive\n",
              "6940  楽天トラベルスーパーSALE開催決定！✅9月4日20時～✅宿クーポン割引率最大28％✅注目は...         0.1  Positive\n",
              "6941  ✅人気のブランドを安く欲しい✅最近アウトレットモールがいまいちだなと思う方ぜひ見てくださいブ...         0.3  Positive\n",
              "6942  majestouchお得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https...         0.1  Positive\n",
              "6943  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/eD...         0.1  Positive\n",
              "6944  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/89...         0.1  Positive\n",
              "6945  自転車!!楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/Kibm...         0.1  Positive\n",
              "6946  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/fL...         0.1  Positive\n",
              "6947  楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/daPe7PIgm...         0.1  Positive\n",
              "6948  積み木と子供のお洋服ほしい〜！お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの ...         0.1  Positive\n",
              "6949  楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/IYFi0BI4m...         0.1  Positive\n",
              "6950  ペットモニター！楽天スーパーSALEでほしいものをつぶやこう！ https://t.co/E...         0.1  Positive\n",
              "6951  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/XB...         0.1  Positive\n",
              "6952  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/2s...         0.1  Positive\n",
              "6953  お得なクーポンがほしい！ #楽天スーパーSALEでほしいもの https://t.co/A5...         0.1  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrUE6L5HZnTe"
      },
      "source": [
        "twitterdata1.to_csv('CNNAnalysis0.csv',encoding=result['encoding'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9_T31LRcFro",
        "outputId": "a605d1fa-fd33-41be-ad4b-e2e327e83c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "twitterdata1.Label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Negative    3508\n",
              "Positive    3446\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    }
  ]
}